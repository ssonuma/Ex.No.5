

# EXP 5: COMPARATIVE ANALYSIS OF DIFFERENT TYPES OF PROMPTING PATTERNS AND EXPLAIN WITH VARIOUS TEST SCENARIOS
# REG NO:212223220107

# Aim: To test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios.  Analyze the quality, accuracy, and depth of the generated responses 

### AI Tools Required: 

# Explanation: 
Define the Two Prompt Types:

Write a basic Prompt: Clear, detailed, and structured prompts that give specific instructions or context to guide the model.
Based on that pattern type refined the prompt and submit that with AI tool.
Get the ouput and write the report.

Prepare Multiple Test Scenarios:
Select various scenarios such as:
Generating a creative story.
Answering a factual question.
Summarizing an article or concept.
Providing advice or recommendations.
Or Any other test scenario
For each scenario, create both a naïve and a basic prompt. Ensure each pair of prompts targets the same task but with different levels of structure.
Run Experiments with ChatGPT:
Input the naïve prompt for each scenario and record the generated response.
Then input the corresponding basic prompt and capture that response.
Repeat this process for all selected scenarios to gather a full set of results.
Evaluate Responses : 
	Compare how ChatGPT performs when given naïve versus basic prompts and analyze the output based on Quality,Accuracy and Depth. Also analyse does ChatGPT consistently provide better results with basic prompts? Are there scenarios where naïve prompts work equally well?
Deliverables:
A table comparing ChatGPT's responses to naïve and basic prompts across all scenarios.
Analysis of how prompt clarity impacts the quality, accuracy, and depth of ChatGPT’s outputs.
Summary of findings with insights on how to structure prompts for optimal results when using ChatGPT.


# OUTPUT
1. Objective

The goal of this experiment is to understand how prompt clarity and structure affect the quality, accuracy, and depth of ChatGPT’s responses.
Two types of prompts were tested:

Prompt Type	Definition
Naïve Prompt	A simple or vague question with little context or structure.
Basic Prompt	A clear, detailed, and structured prompt that gives context, instructions, and expected output format.
2. Methodology

For each scenario, two prompts were designed:

One naïve prompt

One basic structured prompt

Then, both prompts were given to ChatGPT, and the responses were recorded and analyzed based on:

Quality (clarity, readability)

Accuracy (correctness of information)

Depth (detail and explanation level)

3. Test Scenarios and Results
Scenario	Naïve Prompt	Basic Prompt	Naïve Response (Summary)	Basic Response (Summary)	Evaluation
1. Creative Story	“Write a story.”	“Write a 200-word creative story about a lonely robot who learns to make friends, using emotional language and a hopeful ending.”	Very short and random story, lacked emotion or theme.	Detailed, emotional, structured story with clear beginning, middle, and end.	Basic prompt produced richer and more engaging story.
2. Factual Question	“What is AI?”	“Explain Artificial Intelligence in simple terms, including its definition, types, and one real-world example.”	Gave one-line definition.	Gave definition, types (Narrow, General AI), and an example (Siri).	Basic prompt gave complete and accurate answer.
3. Summarization	“Summarize cyber security.”	“Summarize the concept of cybersecurity in 100 words, mentioning its purpose, main threats, and importance.”	Very general summary, missing key points.	Well-organized summary covering purpose, threats, and importance.	Basic prompt gave concise and informative summary.
4. Advice/Recommendation	“How can I study better?”	“Give 5 practical study tips for college students to improve focus and memory, with brief explanations.”	Generic advice like ‘study regularly’.	Structured list with 5 actionable tips and reasons.	Basic prompt gave useful, clear, and specific advice.
5. Explanation of Concept	“Explain cloud computing.”	“Explain the concept of cloud computing, including its definition, types (IaaS, PaaS, SaaS), and one example of each.”	Short, vague definition.	Detailed, structured explanation with examples.	Basic prompt gave accurate and detailed explanation.
4. Analysis
Observations:

In all test cases, the basic prompts produced longer, clearer, and more accurate answers.

Naïve prompts led to short, incomplete, or off-topic responses.

Basic prompts guided ChatGPT to focus on the required structure and depth.

Evaluation by Criteria:
Criteria	Naïve Prompts	Basic Prompts
Quality	Low – vague answers	High – clear, well-organized
Accuracy	Moderate	High
Depth	Shallow	Detailed and thoughtful
Key Insights:

Clear instructions = better results.

Adding structure (like “give 5 points” or “explain in 100 words”) makes responses more consistent.

Context helps ChatGPT understand your intent better.

Naïve prompts can work well only for very simple tasks (like “tell a joke”).

5. Summary of Findings

Prompt clarity significantly affects output quality.

Basic prompts consistently produced more detailed, accurate, and context-aware answers.

To get the best results, prompts should:

Be specific about the task

Include context and instructions

Mention desired format or length

Avoid vague wording

6. Conclusion

This experiment clearly shows that ChatGPT performs best when guided by structured prompts.
For users, this means spending time crafting a clear, detailed prompt leads to higher-quality outputs — whether for stories, summaries, explanations, or advice.

# RESULT: The prompt for the above said problem executed successfully
